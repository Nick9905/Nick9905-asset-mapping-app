import streamlit as st
import pandas as pd
import json
import os
from datetime import datetime
import io
from functools import lru_cache

# é¡µé¢é…ç½®
st.set_page_config(
  page_title="èµ„äº§æ˜ å°„å…³ç³»æŸ¥è¯¢ç³»ç»Ÿ",
  page_icon="ğŸ”—",
  layout="wide"
)

# æ•°æ®æ–‡ä»¶è·¯å¾„
FINANCIAL_DATA_FILE = "financial_assets.json"
PHYSICAL_DATA_FILE = "physical_assets.json"
MAPPING_DATA_FILE = "asset_mapping.json"

# ========== ç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ– ==========

@st.cache_data(ttl=300)  # ç¼“å­˜5åˆ†é’Ÿ
def load_data_cached(file_path):
  """åŠ è½½æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
  if os.path.exists(file_path):
      try:
          with open(file_path, 'r', encoding='utf-8') as f:
              return json.load(f)
      except:
          return []
  return []

def load_data(file_path):
  """åŠ è½½æ•°æ®"""
  return load_data_cached(file_path)

def save_data(data, file_path):
  """ä¿å­˜æ•°æ®å¹¶æ¸…é™¤ç¼“å­˜"""
  with open(file_path, 'w', encoding='utf-8') as f:
      json.dump(data, f, ensure_ascii=False, indent=2, default=str)
  # æ¸…é™¤ç¼“å­˜ä»¥ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
  load_data_cached.clear()

@st.cache_data
def create_mapping_index(mapping_data):
  """åˆ›å»ºæ˜ å°„ç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ•ˆç‡"""
  financial_to_physical = {}
  physical_to_financial = {}
  
  for mapping in mapping_data:
      financial_code = mapping.get("è´¢åŠ¡ç³»ç»Ÿç¼–å·")
      physical_code = mapping.get("å®ç‰©å°è´¦ç¼–å·")
      
      if financial_code and physical_code:
          financial_to_physical[financial_code] = physical_code
          physical_to_financial[physical_code] = financial_code
  
  return financial_to_physical, physical_to_financial

@st.cache_data
def create_data_index(data, key_field):
  """åˆ›å»ºæ•°æ®ç´¢å¼•"""
  return {item[key_field]: item for item in data if item.get(key_field)}

# ========== æ•°æ®å¤„ç†å‡½æ•° ==========

def safe_str_convert(value):
  """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
  if pd.isna(value) or value is None:
      return ""
  if isinstance(value, (datetime, pd.Timestamp)):
      try:
          return value.strftime("%Y-%m-%d")
      except:
          return str(value)
  return str(value).strip()

def safe_float_convert(value):
  """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
  if pd.isna(value) or value is None:
      return 0.0
  if isinstance(value, str):
      cleaned = value.replace(',', '').replace('Â¥', '').replace('$', '').replace(' ', '')
      if cleaned == '' or cleaned == '-':
          return 0.0
      try:
          return float(cleaned)
      except ValueError:
          return 0.0
  if isinstance(value, (int, float)):
      return float(value)
  if isinstance(value, (datetime, pd.Timestamp)):
      return 0.0
  return 0.0

def update_data_generic(existing_data, new_data, key_field):
  """é€šç”¨æ•°æ®æ›´æ–°å‡½æ•°"""
  existing_dict = {item[key_field]: item for item in existing_data if item.get(key_field)}
  
  updated_count = 0
  new_count = 0
  
  for new_item in new_data:
      key_value = new_item.get(key_field)
      if key_value and key_value in existing_dict:
          existing_dict[key_value].update(new_item)
          existing_dict[key_value]["æ›´æ–°æ—¶é—´"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
          updated_count += 1
      elif key_value:
          existing_dict[key_value] = new_item
          new_count += 1
  
  st.info(f"ğŸ“Š æ›´æ–°ç»Ÿè®¡ï¼šæ›´æ–° {updated_count} æ¡ï¼Œæ–°å¢ {new_count} æ¡")
  return list(existing_dict.values())

# ========== æ•°æ®å¯¼å…¥å‡½æ•° ==========

def import_financial_data(df):
  """å¯¼å…¥è´¢åŠ¡ç³»ç»Ÿæ•°æ®"""
  processed_data = []
  
  for index, row in df.iterrows():
      try:
          financial_record = {
              "è´¢åŠ¡ç³»ç»Ÿç¼–å·": safe_str_convert(row.iloc[0] if len(row) > 0 else ""),
              "åºå·": safe_str_convert(row.iloc[1] if len(row) > 1 else ""),
              "æ‰€å±å…¬å¸": safe_str_convert(row.iloc[2] if len(row) > 2 else ""),
              "èµ„äº§åˆ†ç±»": safe_str_convert(row.iloc[3] if len(row) > 3 else ""),
              "èµ„äº§ç¼–å·": safe_str_convert(row.iloc[4] if len(row) > 4 else ""),
              "èµ„äº§åç§°": safe_str_convert(row.iloc[5] if len(row) > 5 else ""),
              "èµ„äº§è§„æ ¼": safe_str_convert(row.iloc[6] if len(row) > 6 else ""),
              "å–å¾—æ—¥æœŸ": safe_str_convert(row.iloc[9] if len(row) > 9 else ""),
              "èµ„äº§ä»·å€¼": safe_float_convert(row.iloc[24] if len(row) > 24 else 0),
              "ç´¯ç§¯æŠ˜æ—§": safe_float_convert(row.iloc[26] if len(row) > 26 else 0),
              "è´¦é¢ä»·å€¼": safe_float_convert(row.iloc[27] if len(row) > 27 else 0),
              "éƒ¨é—¨åç§°": safe_str_convert(row.iloc[36] if len(row) > 36 else ""),
              "ä¿ç®¡äºº": safe_str_convert(row.iloc[38] if len(row) > 38 else ""),
              "å¤‡æ³¨": safe_str_convert(row.iloc[28] if len(row) > 28 else ""),
              "å¯¼å…¥æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
              "åŸå§‹è¡Œå·": index + 1
          }
          
          if financial_record["è´¢åŠ¡ç³»ç»Ÿç¼–å·"] or financial_record["èµ„äº§ç¼–å·"]:
              processed_data.append(financial_record)
      except Exception as e:
          continue
  return processed_data

def import_physical_data(df):
  """å¯¼å…¥å®ç‰©å°è´¦æ•°æ®"""
  processed_data = []
  for index, row in df.iterrows():
      try:
          physical_record = {
              "æ‰€å±å…¬å¸": safe_str_convert(row.iloc[0] if len(row) > 0 else ""),
              "ä¸€çº§éƒ¨é—¨": safe_str_convert(row.iloc[1] if len(row) > 1 else ""),
              "å›ºå®šèµ„äº§ç¼–å·": safe_str_convert(row.iloc[2] if len(row) > 2 else ""),
              "åŸå›ºå®šèµ„äº§ç¼–å·": safe_str_convert(row.iloc[3] if len(row) > 3 else ""),
              "å›ºå®šèµ„äº§ç±»å‹": safe_str_convert(row.iloc[4] if len(row) > 4 else ""),
              "å›ºå®šèµ„äº§åç§°": safe_str_convert(row.iloc[5] if len(row) > 5 else ""),
              "è§„æ ¼å‹å·": safe_str_convert(row.iloc[6] if len(row) > 6 else ""),
              "å­˜æ”¾éƒ¨é—¨": safe_str_convert(row.iloc[7] if len(row) > 7 else ""),
              "åœ°ç‚¹": safe_str_convert(row.iloc[8] if len(row) > 8 else ""),
              "ä½¿ç”¨äºº": safe_str_convert(row.iloc[9] if len(row) > 9 else ""),
              "ä¿ç®¡äºº": safe_str_convert(row.iloc[10] if len(row) > 10 else ""),
              "å®ç›˜æ•°é‡": safe_str_convert(row.iloc[11] if len(row) > 11 else ""),
              "å…¥è´¦æ—¥æœŸ": safe_str_convert(row.iloc[13] if len(row) > 13 else ""),
              "èµ„äº§ä»·å€¼": safe_float_convert(row.iloc[16] if len(row) > 16 else 0),
              "ç´¯è®¡æŠ˜æ—§é¢": safe_float_convert(row.iloc[20] if len(row) > 20 else 0),
              "ä½¿ç”¨çŠ¶æ€": safe_str_convert(row.iloc[21] if len(row) > 21 else ""),
              "å¯¼å…¥æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
              "åŸå§‹è¡Œå·": index + 1
          }
          if physical_record["å›ºå®šèµ„äº§ç¼–å·"]:
              processed_data.append(physical_record)
      except Exception as e:
          continue
  return processed_data

def import_mapping_data(df):
  """å¯¼å…¥å…³ç³»å¯¹ç…§è¡¨æ•°æ®"""
  processed_data = []
  for index, row in df.iterrows():
      try:
          mapping_record = {
              "å®ç‰©å°è´¦ç¼–å·": safe_str_convert(row.iloc[0] if len(row) > 0 else ""),
              "è´¢åŠ¡ç³»ç»Ÿç¼–å·": safe_str_convert(row.iloc[1] if len(row) > 1 else ""),
              "èµ„äº§ç¼–å·": safe_str_convert(row.iloc[2] if len(row) > 2 else ""),
              "å¯¼å…¥æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
              "åŸå§‹è¡Œå·": index + 1
          }
          if mapping_record["å®ç‰©å°è´¦ç¼–å·"] or mapping_record["è´¢åŠ¡ç³»ç»Ÿç¼–å·"]:
              processed_data.append(mapping_record)
      except Exception as e:
          continue
  return processed_data

# ========== é¡µé¢å‡½æ•° ==========

def data_import_page():
  """æ•°æ®å¯¼å…¥é¡µé¢"""
  st.header("ğŸ“¥ æ•°æ®å¯¼å…¥")
  
  tab1, tab2, tab3 = st.tabs(["ğŸ“Š è´¢åŠ¡ç³»ç»Ÿæ•°æ®", "ğŸ“‹ å®ç‰©å°è´¦æ•°æ®", "ğŸ”— å¯¹åº”å…³ç³»æ•°æ®"])
  
  with tab1:
      st.subheader("è´¢åŠ¡ç³»ç»Ÿæ•°æ®å¯¼å…¥")
      financial_file = st.file_uploader("é€‰æ‹©è´¢åŠ¡ç³»ç»ŸExcelæ–‡ä»¶", type=['xlsx', 'xls'], key="financial")
      
      if financial_file is not None:
          try:
              df = pd.read_excel(financial_file)
              st.success("âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
              
              with st.expander("æ•°æ®é¢„è§ˆ", expanded=False):
                  st.dataframe(df.head(), use_container_width=True)
              
              if st.button("ç¡®è®¤å¯¼å…¥è´¢åŠ¡æ•°æ®", type="primary"):
                  with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®..."):
                      processed_data = import_financial_data(df)
                      
                      if processed_data:
                          existing_data = load_data(FINANCIAL_DATA_FILE)
                          updated_data = update_data_generic(existing_data, processed_data, "è´¢åŠ¡ç³»ç»Ÿç¼–å·")
                          save_data(updated_data, FINANCIAL_DATA_FILE)
                          st.success(f"âœ… æˆåŠŸå¯¼å…¥ {len(processed_data)} æ¡è´¢åŠ¡æ•°æ®ï¼")
                          st.rerun()
                      else:
                          st.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å¯¼å…¥")
                          
          except Exception as e:
              st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")
  
  with tab2:
      st.subheader("å®ç‰©å°è´¦æ•°æ®å¯¼å…¥")
      physical_file = st.file_uploader("é€‰æ‹©å®ç‰©å°è´¦Excelæ–‡ä»¶", type=['xlsx', 'xls'], key="physical")
      
      if physical_file is not None:
          try:
              df = pd.read_excel(physical_file)
              st.success("âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
              
              with st.expander("æ•°æ®é¢„è§ˆ", expanded=False):
                  st.dataframe(df.head(), use_container_width=True)
              
              if st.button("ç¡®è®¤å¯¼å…¥å®ç‰©æ•°æ®", type="primary"):
                  with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®..."):
                      processed_data = import_physical_data(df)
                      
                      if processed_data:
                          existing_data = load_data(PHYSICAL_DATA_FILE)
                          updated_data = update_data_generic(existing_data, processed_data, "å›ºå®šèµ„äº§ç¼–å·")
                          save_data(updated_data, PHYSICAL_DATA_FILE)
                          st.success(f"âœ… æˆåŠŸå¯¼å…¥ {len(processed_data)} æ¡å®ç‰©æ•°æ®ï¼")
                          st.rerun()
                      else:
                          st.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å¯¼å…¥")
                          
          except Exception as e:
              st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")
  
  with tab3:
      st.subheader("å¯¹åº”å…³ç³»æ•°æ®å¯¼å…¥")
      mapping_file = st.file_uploader("é€‰æ‹©å¯¹åº”å…³ç³»Excelæ–‡ä»¶", type=['xlsx', 'xls'], key="mapping")
      
      if mapping_file is not None:
          try:
              df = pd.read_excel(mapping_file)
              st.success("âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
              
              with st.expander("æ•°æ®é¢„è§ˆ", expanded=False):
                  st.dataframe(df.head(), use_container_width=True)
              
              if st.button("ç¡®è®¤å¯¼å…¥å¯¹åº”å…³ç³»", type="primary"):
                  with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®..."):
                      processed_data = import_mapping_data(df)
                      
                      if processed_data:
                          existing_data = load_data(MAPPING_DATA_FILE)
                          updated_data = update_data_generic(existing_data, processed_data, "å®ç‰©å°è´¦ç¼–å·")
                          save_data(updated_data, MAPPING_DATA_FILE)
                          st.success(f"âœ… æˆåŠŸå¯¼å…¥ {len(processed_data)} æ¡å¯¹åº”å…³ç³»ï¼")
                          st.rerun()
                      else:
                          st.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å¯¼å…¥")
                          
          except Exception as e:
              st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")

def mapping_query_page():
  """æ˜ å°„å…³ç³»æŸ¥è¯¢é¡µé¢"""
  st.header("ğŸ” èµ„äº§æ˜ å°„å…³ç³»æŸ¥è¯¢")
  st.info("è¯·å…ˆå¯¼å…¥æ•°æ®åå†è¿›è¡ŒæŸ¥è¯¢")

def data_statistics_page():
  """æ•°æ®ç»Ÿè®¡åˆ†æé¡µé¢"""
  st.header("ğŸ“Š æ•°æ®ç»Ÿè®¡åˆ†æ")
  st.info("è¯·å…ˆå¯¼å…¥æ•°æ®åå†æŸ¥çœ‹ç»Ÿè®¡")

def all_data_view_page():
  """æŸ¥çœ‹å…¨éƒ¨å¯¹åº”å…³ç³»é¡µé¢"""
  st.header("ğŸ“‹ å…¨éƒ¨èµ„äº§å¯¹åº”å…³ç³»")
  st.info("è¯·å…ˆå¯¼å…¥æ•°æ®åå†æŸ¥çœ‹è¯¦æƒ…")

# ========== ä¸»å‡½æ•° ==========

def main():
  """ä¸»å‡½æ•°"""
  st.title("ğŸ”— èµ„äº§æ˜ å°„å…³ç³»æŸ¥è¯¢ç³»ç»Ÿ")
  
  # ä¾§è¾¹æ å¯¼èˆª
  with st.sidebar:
      st.header("ğŸ“‹ ç³»ç»Ÿå¯¼èˆª")
      page = st.selectbox(
          "é€‰æ‹©åŠŸèƒ½é¡µé¢",
          ["ğŸ“¥ æ•°æ®å¯¼å…¥", "ğŸ” æ˜ å°„æŸ¥è¯¢", "ğŸ“Š æ•°æ®ç»Ÿè®¡", "ğŸ“‹ å…¨éƒ¨æ•°æ®"],
          key="page_selector"
      )
      
      st.markdown("---")
      st.markdown("### ğŸ“ ä½¿ç”¨è¯´æ˜")
      st.markdown("""
      1. **æ•°æ®å¯¼å…¥**ï¼šä¸Šä¼ Excelæ–‡ä»¶å¯¼å…¥æ•°æ®
      2. **æ˜ å°„æŸ¥è¯¢**ï¼šæŸ¥è¯¢èµ„äº§å¯¹åº”å…³ç³»
      3. **æ•°æ®ç»Ÿè®¡**ï¼šæŸ¥çœ‹ç»Ÿè®¡åˆ†æç»“æœ
      4. **å…¨éƒ¨æ•°æ®**ï¼šæµè§ˆæ‰€æœ‰æ•°æ®è®°å½•
      """)
  
  # æ ¹æ®é€‰æ‹©æ˜¾ç¤ºå¯¹åº”é¡µé¢
  if page == "ğŸ“¥ æ•°æ®å¯¼å…¥":
      data_import_page()
  elif page == "ğŸ” æ˜ å°„æŸ¥è¯¢":
      mapping_query_page()
  elif page == "ğŸ“Š æ•°æ®ç»Ÿè®¡":
      data_statistics_page()
  elif page == "ğŸ“‹ å…¨éƒ¨æ•°æ®":
      all_data_view_page()

# ========== ç¨‹åºå…¥å£ ==========

if __name__ == "__main__":
    main()
